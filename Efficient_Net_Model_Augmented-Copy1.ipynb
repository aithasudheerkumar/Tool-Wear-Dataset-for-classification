{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb376c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chipping', 'Crater', 'Flank']\n",
      "Types of classes labels found:  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "#dataset = 'G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/I_Dataset/'\n",
    "dataset_path = os.listdir('G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/')\n",
    "\n",
    "print (dataset_path)  #what kinds of classes are in this dataset\n",
    "\n",
    "print(\"Types of classes labels found: \", len(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91492ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_classes = os.listdir('G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/' + '/' +item)\n",
    " #print(all_classes)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_classes:\n",
    "    class_labels.append((item, str('dataset_path' + '/' +item) + '/' + room))\n",
    "    #print(class_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb29889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Labels                         image\n",
      "0  Chipping   dataset_path/Chipping/6.jpg\n",
      "1  Chipping  dataset_path/Chipping/12.jpg\n",
      "2  Chipping  dataset_path/Chipping/18.jpg\n",
      "3  Chipping  dataset_path/Chipping/24.jpg\n",
      "4  Chipping  dataset_path/Chipping/30.jpg\n",
      "     Labels                        image\n",
      "3103  Flank  dataset_path/Flank/0995.png\n",
      "3104  Flank  dataset_path/Flank/0996.png\n",
      "3105  Flank  dataset_path/Flank/0997.png\n",
      "3106  Flank  dataset_path/Flank/0998.png\n",
      "3107  Flank  dataset_path/Flank/0999.png\n"
     ]
    }
   ],
   "source": [
    "# Build a dataframe        \n",
    "df = pd.DataFrame(data=class_labels, columns=['Labels', 'image'])\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d52f9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in the dataset:  3108\n",
      "Chipping    1036\n",
      "Crater      1036\n",
      "Flank       1036\n",
      "Name: Labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's check how many samples for each category are present\n",
    "print(\"Total number of images in the dataset: \", len(df))\n",
    "\n",
    "label_count = df['Labels'].value_counts()\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e03320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "path = 'G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/'\n",
    "dataset_path = os.listdir('G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/')\n",
    "\n",
    "im_size = 300\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i in dataset_path:\n",
    "    data_path = path + str(i)  \n",
    "    filenames = [i for i in os.listdir(data_path) ]\n",
    "   \n",
    "    for f in filenames:\n",
    "        img = cv2.imread(data_path + '/' + f)\n",
    "        img = cv2.resize(img, (im_size, im_size))\n",
    "        images.append(img)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa03febf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3108, 300, 300, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This model takes input images of shape (224, 224, 3), and the input data should range [0, 255]. \n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "images = images.astype('float32') / 255.0\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd53444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chipping' 'Chipping' 'Chipping' ... 'Flank' 'Flank' 'Flank']\n",
      "[0 0 0 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "y=df['Labels'].values\n",
    "print(y)\n",
    "\n",
    "y_labelencoder = LabelEncoder ()\n",
    "y = y_labelencoder.fit_transform (y)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d34bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y=y.reshape(-1,1)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "Y = ct.fit_transform(y) #.toarray()\n",
    "print(Y[:5])\n",
    "print(Y[35:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e058dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2486, 300, 300, 3)\n",
      "(2486, 1)\n",
      "(622, 300, 300, 3)\n",
      "(622, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "images, Y = shuffle(images, y, random_state=1)\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.2, random_state=415)\n",
    "\n",
    "#inpect the shape of the training and testing.\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1106f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tf.keras.utils.to_categorical(train_y, 3)\n",
    "test_y = tf.keras.utils.to_categorical(test_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7916bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fda42206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IMG_SIZE = 300\n",
    "size = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "\n",
    "# Using model without transfer learning\n",
    "\n",
    "outputs = EfficientNetB3(include_top=True, weights=None, classes=NUM_CLASSES)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01434ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 300, 300, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb3 (Functional)  (None, 3)                10788146  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,788,146\n",
      "Trainable params: 10,700,843\n",
      "Non-trainable params: 87,303\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "78/78 - 1550s - loss: 1.3767 - accuracy: 0.6677 - 1550s/epoch - 20s/step\n",
      "Epoch 2/25\n",
      "78/78 - 1490s - loss: 0.4265 - accuracy: 0.8805 - 1490s/epoch - 19s/step\n",
      "Epoch 3/25\n",
      "78/78 - 1512s - loss: 0.1494 - accuracy: 0.9525 - 1512s/epoch - 19s/step\n",
      "Epoch 4/25\n",
      "78/78 - 1475s - loss: 0.0921 - accuracy: 0.9682 - 1475s/epoch - 19s/step\n",
      "Epoch 5/25\n",
      "78/78 - 1603s - loss: 0.2503 - accuracy: 0.9525 - 1603s/epoch - 21s/step\n",
      "Epoch 6/25\n",
      "78/78 - 1468s - loss: 0.0809 - accuracy: 0.9771 - 1468s/epoch - 19s/step\n",
      "Epoch 7/25\n",
      "78/78 - 1456s - loss: 0.0649 - accuracy: 0.9819 - 1456s/epoch - 19s/step\n",
      "Epoch 8/25\n",
      "78/78 - 1587s - loss: 0.0850 - accuracy: 0.9795 - 1587s/epoch - 20s/step\n",
      "Epoch 9/25\n",
      "78/78 - 1626s - loss: 0.0509 - accuracy: 0.9839 - 1626s/epoch - 21s/step\n",
      "Epoch 10/25\n",
      "78/78 - 1494s - loss: 0.2207 - accuracy: 0.9372 - 1494s/epoch - 19s/step\n",
      "Epoch 11/25\n",
      "78/78 - 1513s - loss: 0.1637 - accuracy: 0.9509 - 1513s/epoch - 19s/step\n",
      "Epoch 12/25\n",
      "78/78 - 1468s - loss: 0.0986 - accuracy: 0.9722 - 1468s/epoch - 19s/step\n",
      "Epoch 13/25\n",
      "78/78 - 1540s - loss: 0.0950 - accuracy: 0.9726 - 1540s/epoch - 20s/step\n",
      "Epoch 14/25\n",
      "78/78 - 1491s - loss: 0.0379 - accuracy: 0.9883 - 1491s/epoch - 19s/step\n",
      "Epoch 15/25\n",
      "78/78 - 1534s - loss: 0.0387 - accuracy: 0.9895 - 1534s/epoch - 20s/step\n",
      "Epoch 16/25\n",
      "78/78 - 1525s - loss: 0.0235 - accuracy: 0.9924 - 1525s/epoch - 20s/step\n",
      "Epoch 17/25\n",
      "78/78 - 1515s - loss: 0.0241 - accuracy: 0.9932 - 1515s/epoch - 19s/step\n",
      "Epoch 18/25\n",
      "78/78 - 1464s - loss: 0.0300 - accuracy: 0.9932 - 1464s/epoch - 19s/step\n",
      "Epoch 19/25\n",
      "78/78 - 1552s - loss: 0.0153 - accuracy: 0.9960 - 1552s/epoch - 20s/step\n",
      "Epoch 20/25\n",
      "78/78 - 1457s - loss: 0.0159 - accuracy: 0.9952 - 1457s/epoch - 19s/step\n",
      "Epoch 21/25\n",
      "78/78 - 1489s - loss: 0.0148 - accuracy: 0.9968 - 1489s/epoch - 19s/step\n",
      "Epoch 22/25\n",
      "78/78 - 1470s - loss: 0.0149 - accuracy: 0.9980 - 1470s/epoch - 19s/step\n",
      "Epoch 23/25\n",
      "78/78 - 1448s - loss: 0.0225 - accuracy: 0.9932 - 1448s/epoch - 19s/step\n",
      "Epoch 24/25\n",
      "78/78 - 1424s - loss: 0.0375 - accuracy: 0.9912 - 1424s/epoch - 18s/step\n",
      "Epoch 25/25\n",
      "78/78 - 1534s - loss: 0.0427 - accuracy: 0.9883 - 1534s/epoch - 20s/step\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"] )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=25, verbose=2)c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a7ebe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('Efficient_Aug_25-1epochs.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d3d4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "my_model = load_model('Efficient_Aug_25-1epochs.hdf5', \n",
    "                      compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81be743",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5b31338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eefc5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c61cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 1, 1, 2,\n",
       "       1, 2, 1, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1,\n",
       "       2, 0, 2, 0, 0, 0, 0, 1, 2, 0, 2, 1, 0, 2, 0, 0, 0, 1, 2, 1, 0, 1,\n",
       "       1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 0, 2, 1, 1, 0, 1, 1, 0, 1,\n",
       "       2, 0, 2, 0, 0, 1, 2, 1, 0, 0, 1, 2, 2, 0, 0, 0, 2, 2, 2, 1, 1, 0,\n",
       "       1, 2, 2, 1, 1, 1, 2, 0, 2, 1, 0, 0, 1, 0, 2, 2, 0, 0, 0, 2, 2, 0,\n",
       "       2, 0, 0, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 0, 2, 0, 1, 0, 0,\n",
       "       0, 2, 2, 2, 0, 2, 0, 1, 2, 0, 0, 2, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2,\n",
       "       2, 2, 2, 0, 0, 2, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 2, 2,\n",
       "       0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 0, 1, 1, 0,\n",
       "       0, 2, 0, 1, 2, 1, 1, 1, 1, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2,\n",
       "       1, 2, 1, 2, 2, 2, 1, 1, 0, 0, 2, 0, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1,\n",
       "       0, 1, 0, 0, 1, 2, 2, 2, 1, 0, 2, 2, 2, 1, 0, 2, 1, 1, 1, 1, 0, 2,\n",
       "       1, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 1, 2, 1, 1, 1, 2, 2, 0, 2, 2, 1,\n",
       "       0, 0, 2, 2, 1, 0, 2, 2, 1, 1, 0, 0, 2, 1, 1, 2, 1, 2, 0, 1, 0, 1,\n",
       "       2, 1, 0, 2, 2, 0, 2, 0, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 0,\n",
       "       2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 2, 1, 1, 1, 2, 1, 0,\n",
       "       0, 2, 0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 2, 2, 1, 1, 2, 2, 0, 2, 2, 1,\n",
       "       0, 2, 1, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 2, 0, 2, 2, 2, 1, 0, 2, 0,\n",
       "       0, 1, 1, 1, 2, 2, 1, 0, 2, 0, 2, 0, 1, 2, 0, 2, 1, 0, 1, 0, 2, 0,\n",
       "       1, 0, 1, 2, 0, 2, 2, 0, 1, 2, 0, 2, 1, 2, 2, 0, 2, 1, 1, 2, 0, 1,\n",
       "       0, 1, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1, 0, 1, 0, 2, 1, 0, 0, 2,\n",
       "       2, 0, 0, 2, 0, 0, 0, 1, 2, 0, 2, 1, 0, 1, 2, 2, 1, 2, 0, 2, 0, 1,\n",
       "       0, 0, 1, 0, 2, 2, 2, 0, 2, 2, 1, 2, 0, 0, 1, 1, 0, 1, 1, 2, 0, 2,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 1, 2, 2, 0, 2, 1, 2, 2,\n",
       "       0, 0, 2, 0, 1, 1, 2, 0, 0, 0, 1, 2, 2, 1, 2, 2, 0, 1, 2, 2, 2, 0,\n",
       "       2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 2,\n",
       "       1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0,\n",
       "       0, 0, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rounded_labels=np.argmax(test_y, axis=1)\n",
    "rounded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "543e2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimator:\n",
    "    _estimator_type = ''\n",
    "    classes_=[]\n",
    "    def __init__(self, model, classes):\n",
    "        self.model = model\n",
    "        self._estimator_type = 'classifier'\n",
    "        self.classes_ = classes\n",
    "    def predict(self, X):\n",
    "        y_prob= self.model.predict(X)\n",
    "        y_pred = y_prob.argmax(axis=1)\n",
    "        return y_pred\n",
    "\n",
    "classifier = estimator(model, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d29e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aitha Sudheer Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 87s 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x20380afff40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(estimator=classifier, X=test_x, y_true=rounded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "409d4163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 82s 4s/step - loss: 0.2093 - accuracy: 0.9952\n",
      "Loss = 0.20925621688365936\n",
      "Test Accuracy = 0.9951768517494202\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(test_x, test_y)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b88b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    #plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4cba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ba888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "img_path = 'G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/I_Dataset/Flank/100.jpg'\n",
    "\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img, (300, 300))\n",
    "\n",
    "x = np.expand_dims(img, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "print('Input image shape:', x.shape)\n",
    "\n",
    "my_image = imread(img_path)\n",
    "imshow(img)\n",
    "\n",
    "\n",
    "\n",
    "preds=model.predict(x)\n",
    "print(\"predicted class: \", preds )    # probabilities for being in each of the 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "testY = to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525a9a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e6875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
