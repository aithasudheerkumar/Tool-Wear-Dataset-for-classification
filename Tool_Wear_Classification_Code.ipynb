{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb376c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required libraries and providing the image dataset path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "dataset_path = os.listdir('G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/')\n",
    "\n",
    "print (dataset_path)  #what kinds of classes are in this dataset\n",
    "\n",
    "print(\"Types of classes labels found: \", len(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91492ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class labels for evaluating the loss function (categorical crossentropy)\n",
    "class_labels = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_classes = os.listdir('G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/' + '/' +item)\n",
    " #print(all_classes)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_classes:\n",
    "    class_labels.append((item, str('dataset_path' + '/' +item) + '/' + room))\n",
    "    #print(class_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe        \n",
    "df = pd.DataFrame(data=class_labels, columns=['Labels', 'image'])\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how many samples for each category are present\n",
    "print(\"Total number of images in the dataset: \", len(df))\n",
    "\n",
    "label_count = df['Labels'].value_counts()\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e03320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing all the input images to as per the pre-trained model requirement\n",
    "\n",
    "import cv2\n",
    "path = 'G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/'\n",
    "dataset_path = os.listdir('G:/Other computers/My_Laptop/Objective_1/Semantic Segmentation - U - Net/Efficient_Net_Classification/AI_Dataset/')\n",
    "\n",
    "im_size = 300\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i in dataset_path:\n",
    "    data_path = path + str(i)  \n",
    "    filenames = [i for i in os.listdir(data_path) ]\n",
    "   \n",
    "    for f in filenames:\n",
    "        img = cv2.imread(data_path + '/' + f)\n",
    "        img = cv2.resize(img, (im_size, im_size))\n",
    "        images.append(img)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model takes input images of shape (300, 300, 3), and the input data should range [0, 255]. \n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "images = images.astype('float32') / 255.0\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd53444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the labels to integers values for example chipping - 0, crater - 1, flank - 2\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "y=df['Labels'].values\n",
    "print(y)\n",
    "\n",
    "y_labelencoder = LabelEncoder ()\n",
    "y = y_labelencoder.fit_transform (y)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d34bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.reshape(-1,1)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "Y = ct.fit_transform(y) #.toarray()\n",
    "print(Y[:5])\n",
    "print(Y[35:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training and test dataset of the images\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "images, Y = shuffle(images, y, random_state=1)\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.2, random_state=415)\n",
    "\n",
    "#inspect the shape of the training and testing.\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling of the training data using one hot encoder\n",
    "train_y = tf.keras.utils.to_categorical(train_y, 3)\n",
    "test_y = tf.keras.utils.to_categorical(test_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7916bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda42206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the efficientnetB3 model \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IMG_SIZE = 300\n",
    "size = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "\n",
    "# Using model without transfer learning\n",
    "\n",
    "outputs = EfficientNetB3(include_top=True, weights=None, classes=NUM_CLASSES)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01434ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model compilation, optimiser selection and loss function selection\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"] )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=30, verbose=2)\n",
    "\n",
    "model.save('Efficient_Tool_Wear.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b31338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries to plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rounded_labels=np.argmax(test_y, axis=1)\n",
    "rounded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class estimator for test image dataset\n",
    "class estimator:\n",
    "    _estimator_type = ''\n",
    "    classes_=[]\n",
    "    def __init__(self, model, classes):\n",
    "        self.model = model\n",
    "        self._estimator_type = 'classifier'\n",
    "        self.classes_ = classes\n",
    "    def predict(self, X):\n",
    "        y_prob= self.model.predict(X)\n",
    "        y_pred = y_prob.argmax(axis=1)\n",
    "        return y_pred\n",
    "\n",
    "classifier = estimator(model, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d29e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix plot\n",
    "plot_confusion_matrix(estimator=classifier, X=test_x, y_true=rounded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and test accuracy prediction as per the confusion matrix\n",
    "preds = model.evaluate(test_x, test_y)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4aa8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction code for new images which are not involved in the training\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('Efficient_Tool_Wear_1.hdf5')\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = 'G:/Other computers/My_Laptop/Objective_1/Clemson_Presentation/Testing_dataset/jpeg_images/2.jpg'\n",
    "img = image.load_img(img_path, target_size=(300, 300))\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(img_array)\n",
    "\n",
    "# Print the prediction\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ba888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction code for new images which are not involved in the training\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "img_path = 'G:/Other computers/My_Laptop/Objective_1/Clemson_Presentation/Testing_dataset/jpeg_images/16.jpg'\n",
    "\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img, (300, 300))\n",
    "\n",
    "x = np.expand_dims(img, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "print('Input image shape:', x.shape)\n",
    "\n",
    "my_image = imread(img_path)\n",
    "imshow(img)\n",
    "\n",
    "\n",
    "\n",
    "preds=model.predict(x)\n",
    "print(\"predicted class: \", preds )    # probabilities for being in each of the 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction code for new images which are not involved in the training\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "Img_path = \"G:/Other computers/My_Laptop/Objective_1/Clemson_Presentation/Testing_dataset/jpeg_images/16.jpg\"\n",
    "\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    Img_path, target_size=(300, 300)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
